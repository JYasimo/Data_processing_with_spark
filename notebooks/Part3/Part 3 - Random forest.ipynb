{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b1512f6-c5f4-4a66-a612-600aeaef3484",
     "showTitle": true,
     "title": "Random Forest"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "df_path= \"/dbfs/mnt/bde2/combined_df\"\n",
    "combined_df= spark.read.parquet(df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9665684c-295d-4798-bece-9c72a109708d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: ['VendorID',\n 'tpep_pickup_datetime',\n 'tpep_dropoff_datetime',\n 'store_and_fwd_flag',\n 'RatecodeID',\n 'PULocationID',\n 'DOLocationID',\n 'passenger_count',\n 'trip_distance',\n 'fare_amount',\n 'extra',\n 'mta_tax',\n 'tip_amount',\n 'tolls_amount',\n 'ehail_fee',\n 'improvement_surcharge',\n 'total_amount',\n 'payment_type',\n 'trip_type',\n 'congestion_surcharge',\n 'year',\n 'weekday',\n 'hour',\n 'month',\n 'day_of_year',\n 'trip_duration_sec',\n 'trip_distance_km',\n 'trip_duration_min',\n 'speed',\n 'Borough_pu',\n 'Zone_pu',\n 'service_zone_pu',\n 'Borough_do',\n 'Zone_do',\n 'service_zone_do',\n 'airport',\n 'airport_fee',\n 'colour']"
     ]
    }
   ],
   "source": [
    "#number of columns \n",
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71ffab9a-1ec5-424a-908d-d3d7a73dae8d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data Prepration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01e141d8-15bc-4bff-af24-430e50c003c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Copy df into new df called combined_df_cleaned\n",
    "combined_df_cleaned = combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d6667d5-001d-4aaa-a4b6-a7f13b10908f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of columns to drop\n",
    "columns_to_drop = ['fare_amount', 'trip_duration_sec', 'trip_distance']\n",
    "\n",
    "# Drop the specified columns using the `select` method\n",
    "combined_df_cleaned = combined_df_cleaned.select([col for col in combined_df_cleaned.columns if col not in columns_to_drop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1b681f2-1091-4a6c-bb5e-68f5854cbd68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# wanted columns\n",
    "cols_list=[\n",
    "  'weekday',\n",
    " 'hour',\n",
    " 'year',\n",
    " 'month',\n",
    " 'trip_distance_km',\n",
    " 'trip_duration_min',\n",
    " 'total_amount',\n",
    " 'speed',\n",
    " 'Zone_pu']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21d09198-a492-47c4-a733-d440e926fa30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#subset df\n",
    "combined_df_cleaned = combined_df_cleaned.select(cols_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9963cfa7-1169-4b70-8fbb-ae7cdb0280d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "###Start small to build and validate your code before running on the entire dataset.##\n",
    "######################################################################################\n",
    "\n",
    "num_cols = [\n",
    "  'weekday',\n",
    " 'hour',\n",
    " 'year',\n",
    " 'month',\n",
    " 'trip_distance_km',\n",
    " 'trip_duration_min',\n",
    " 'total_amount',\n",
    " 'speed',]\n",
    "\n",
    "\n",
    "\n",
    "cat_cols =[\n",
    "'service_zone_pu'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79087675-fc22-4021-84b5-228d23816bbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+-----+------------------+------------------+------------+------------------+--------------------+\n|weekday|hour|year|month|  trip_distance_km| trip_duration_min|total_amount|             speed|             Zone_pu|\n+-------+----+----+-----+------------------+------------------+------------+------------------+--------------------+\n|      6|   0|2015|    5|1.7541806000000002| 5.083333333333333|         7.8| 20.70508249180328|Williamsburg (Nor...|\n|      6|   0|2015|    5|         9.2215182|28.816666666666666|       27.96|19.200384916136496|Williamsburg (Nor...|\n|      6|   0|2015|    5|          3.781949|             10.35|        10.8| 21.92434202898551|   East Williamsburg|\n|      6|   0|2015|    5|         4.3130312|             12.65|       15.38|20.457064980237153|      Bushwick South|\n|      6|   0|2015|    5| 7.628271600000001|25.083333333333332|        20.8|18.247028411960137|Williamsburg (Nor...|\n|      6|   0|2015|    5|        10.4124298| 42.56666666666667|        30.8|14.676878339859046|             Bedford|\n|      6|   0|2015|    5|          7.885766|              16.6|       22.56|28.502768674698796|Williamsburg (Nor...|\n|      6|   0|2015|    5|         6.4212666|19.116666666666667|       21.36| 20.15393178727114|          Park Slope|\n|      6|   0|2015|    5|1.9312079999999998| 6.116666666666666|         7.8|18.943729700272478|    Hamilton Heights|\n|      6|   0|2015|    5|1.2874720000000002|3.1166666666666667|         5.8|24.785557219251338|        Forest Hills|\n|      6|   0|2015|    5|        13.5345494|              30.9|        38.5|26.280678446601943|Williamsburg (Nor...|\n|      6|   0|2015|    5| 7.322496999999999|19.433333333333334|       21.96|22.608052487135502|      Bushwick North|\n|      6|   0|2015|    5|4.6348991999999996|14.916666666666666|        13.8|18.643169966480443|  South Williamsburg|\n|      6|   0|2015|    5|         6.2603326|18.283333333333335|        16.8|20.544391394712854|          Greenpoint|\n|      6|   0|2015|    5|          5.713157|             13.15|        17.8|26.067636501901138|Williamsburg (Nor...|\n|      6|   0|2015|    5|1.2874720000000002| 3.966666666666667|         6.3|19.474366386554625|             Astoria|\n|      6|   0|2015|    5|2.2530759999999996| 8.416666666666666|         9.3|16.061531881188117|            Elmhurst|\n|      6|   0|2015|    5|3.1703997999999998| 8.916666666666666|         9.8| 21.33353136448598|Van Cortlandt Vil...|\n|      6|   0|2015|    5|         2.5105704| 7.533333333333333|         8.8|19.995693451327433|University Height...|\n|      6|   0|2015|    5|2.2530759999999996| 4.083333333333333|         8.3| 33.10642285714285|             Bedford|\n+-------+----+----+-----+------------------+------------------+------------+------------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Subset the dataframe with the columns list cols_list\n",
    "combined_df=combined_df.select(cols_list)\n",
    "combined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6881063-2a90-4bfb-9cf9-c3597cec4ad2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "#split the data\n",
    "train_df = combined_df_cleaned.filter(\n",
    "    (col(\"tpep_pickup_datetime\") >= \"2020-01-01\") &\n",
    "    (col(\"tpep_pickup_datetime\") < \"2022-12-31\")\n",
    ")\n",
    "\n",
    "# # Time period 2: From '2022-10-01' to '2022-12-31'\n",
    "# test_df = combined_df_cleaned.filter(\n",
    "#     (col(\"tpep_pickup_datetime\") >= \"2022-10-01\") &\n",
    "#     (col(\"tpep_pickup_datetime\") <= \"2022-12-31\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6612e42-0423-40f0-91c4-1cd4edd7714e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns= 9\nrowss= 84234817\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"columns=\",len(train_df.columns))\n",
    "print(\"rowss=\",train_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28d74f38-7df0-424a-ae0b-ebc6ad432a41",
     "showTitle": true,
     "title": "Pipeline"
    }
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#create empty stage\n",
    "stages = []\n",
    "\n",
    "#Iterate through cat_cols and instantiate StringIndexer and OneHotEncoder for each column and them to stages\n",
    "for cat_col in cat_cols:\n",
    "    col_indexer = StringIndexer(inputCol=cat_col, outputCol=f\"{cat_col}_ind\")\n",
    "    col_encoder = OneHotEncoder(inputCols=[f\"{cat_col}_ind\"], outputCols=[f\"{cat_col}_ohe\"])\n",
    "    stages += [col_indexer, col_encoder]\n",
    "\n",
    "#Create a new list called cat_cols_ohe that will add the suffix _ohe to each element of cat_cols\n",
    "cat_cols_ohe = [f\"{cat_col}_ohe\" for cat_col in cat_cols]\n",
    "\n",
    "#initiate vector assembler\n",
    "assembler = VectorAssembler(inputCols=cat_cols_ohe + num_cols, outputCol=\"features\")\n",
    "\n",
    "#Add assembler to stages\n",
    "stages += [assembler]\n",
    "\n",
    "#pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4447208-00f3-470f-b725-d8af57abe4b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Fit the pipeline with dataframes\n",
    "pipeline_model_train = pipeline.fit(train_df)\n",
    "#pipeline_model_test = pipeline.fit(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "464262aa-11b9-451a-90b0-0576665210bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Apply pipeline to dataframes\n",
    "train_df= pipeline_model_train.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "022c6c04-d7ca-4f21-a11b-d24e1966e719",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#last year\n",
    "from pyspark.sql.functions import col\n",
    "#split the data\n",
    "train_data = train_df.filter(\n",
    "    (col(\"tpep_pickup_datetime\") >= \"2020-01-01\") &\n",
    "    (col(\"tpep_pickup_datetime\") < \"2022-10-01\")\n",
    ")\n",
    "\n",
    "# # Time period 2: From '2022-10-01' to '2022-12-31'\n",
    "test_data = train_df.filter(\n",
    "    (col(\"tpep_pickup_datetime\") >= \"2022-10-01\") &\n",
    "    (col(\"tpep_pickup_datetime\") <= \"2022-12-31\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a13ad3f8-59b4-4c85-bd24-14d4ae7777c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#train LinearRegression\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor(featuresCol='features', labelCol='total_amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "207cbe11-411c-4847-9850-6fc943ec8e16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#fit the model on training set\n",
    "dt_model = dt.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5edc76b3-ea75-4d91-8101-f61b63b2a6cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#make pred on train set\n",
    "train_preds = dt_model.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd28b5ff-d50c-44f5-9052-3ecb2b8ac1a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#show columns vs pred train\n",
    "train_preds.select(cols_list + ['prediction']).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6dae40b-1354-489f-af76-20dd7de3ab38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#make pred on test set\n",
    "test_preds = dt_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9836473b-e484-40c1-9e63-5db4bb05d958",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#show columns vs pred train\n",
    "test_preds.select(cols_list + ['prediction']).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7afa7127-7c96-41a9-83f5-0319b58ec35b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#evaluate\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName='rmse',labelCol='total_amount',predictionCol='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "224f732f-b2c7-4558-98be-90018f43a8b9",
     "showTitle": true,
     "title": "RMSE"
    }
   },
   "outputs": [],
   "source": [
    "#print rmse train\n",
    "rmse_train =evaluator.evaluate(train_preds)\n",
    "print('rmse_train=',rmse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92c6f40b-5b26-4bbc-910e-cd4ce8b6a96d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rmse_test =evaluator.evaluate(test_preds)\n",
    "print('rmse_test=',rmse_test)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Part 3 - Random forest",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
